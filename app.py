import glob
import os
from collections import Counter

import pandas as pd
import streamlit as st

st.set_page_config(page_title="Bulls Fan Revenue Intelligence", layout="wide")

DATA_DIR = "data/comments_by_thread"

st.title("Bulls Fan Revenue Intelligence")
st.caption("Dashboard reads CSVs in data/comments_by_thread (generated by your scraper).")

files = sorted(glob.glob(os.path.join(DATA_DIR, "*.csv")))
if not files:
    st.warning("No CSVs found. Upload or sync data to data/comments_by_thread/")
    st.stop()

@st.cache_data
def load_comments(file_list):
    dfs = []
    for f in file_list:
        df = pd.read_csv(f)
        df["source_file"] = os.path.basename(f)
        dfs.append(df)
    out = pd.concat(dfs, ignore_index=True)

    # basic cleanup
    for col in ["game_date", "thread_type", "thread_id", "author", "body"]:
        if col in out.columns:
            out[col] = out[col].fillna("").astype(str)
    if "score" in out.columns:
        out["score"] = pd.to_numeric(out["score"], errors="coerce").fillna(0).astype(int)

    return out

df = load_comments(files)

# Sidebar filters
st.sidebar.header("Filters")

dates = sorted([d for d in df["game_date"].unique() if d])
selected_dates = st.sidebar.multiselect("Game dates", dates, default=dates)

types = sorted([t for t in df["thread_type"].unique() if t])
selected_types = st.sidebar.multiselect("Thread types", types, default=types)

f = df[df["game_date"].isin(selected_dates) & df["thread_type"].isin(selected_types)].copy()

# Top KPIs
c1, c2, c3, c4 = st.columns(4)
c1.metric("Games", f["game_date"].nunique())
c2.metric("Threads", f["thread_id"].nunique())
c3.metric("Total comments", f"{len(f):,}")
c4.metric("Unique commenters", f["author"].nunique())

st.divider()

# Volume by game + thread type
st.subheader("Comment volume by game and thread type")
vol = (
    f.groupby(["game_date", "thread_type"])
    .size()
    .reset_index(name="comments")
    .sort_values(["game_date", "thread_type"])
)
st.dataframe(vol, use_container_width=True)

st.divider()

# Quick narrative proxy: top tokens
st.subheader("Top recurring words (quick narrative proxy)")

text = " ".join(f["body"].astype(str).tolist()).lower()
stop = {
    "the","and","to","of","a","in","is","it","for","on","that","we","this","with","was","are","as",
    "they","be","but","have","not","at","you","i","our","so","if","just","im","its","its","from"
}
tokens = [t.strip(".,!?()[]{}:;\"'") for t in text.split()]
tokens = [t for t in tokens if len(t) >= 4 and t not in stop]
top = Counter(tokens).most_common(40)
st.table(pd.DataFrame(top, columns=["word", "count"]))

st.divider()

# Raw preview
st.subheader("Raw comments preview")
cols = [c for c in ["game_date", "thread_type", "thread_id", "author", "score", "created_utc", "body", "source_file"] if c in f.columns]
st.dataframe(f[cols].head(200), use_container_width=True)
